# ğŸ“Š ANÃLISIS: QuÃ© Queremos vs QuÃ© Tenemos

**Fecha:** 2025-11-01  
**Objetivo:** Comparar el objetivo final con la realidad actual para identificar gaps y planificar mejoras

---

## ğŸ¯ QUÃ‰ QUEREMOS (Objetivo Final)

### VisiÃ³n Ideal
Un pipeline **simple y directo** como los pipelines estÃ¡ndar de GitHub (nf-core, skipper, etc.):

```
INPUT: Un archivo CSV
  â†“
./run.sh input.csv
  â†“
OUTPUTS: Todas las grÃ¡ficas + tablas + viewers HTML
```

### CaracterÃ­sticas Deseadas

1. **Input Simple y Ãšnico**
   - Un solo archivo CSV como entrada
   - Formato bien documentado
   - ValidaciÃ³n automÃ¡tica del formato

2. **EjecuciÃ³n Simple**
   - Un comando: `./run.sh input.csv`
   - Sin configuraciÃ³n manual necesaria
   - Auto-detecciÃ³n de parÃ¡metros

3. **Output Completo**
   - Todas las figuras (Step 1 + Step 1.5 + Step 2)
   - Todas las tablas CSV
   - Viewers HTML interactivos
   - Todo en directorio organizado

4. **Pipeline GenÃ©rico**
   - Funciona con cualquier dataset (ALS + Control)
   - No hardcodea rutas especÃ­ficas
   - Configurable pero con defaults sensatos

---

## ğŸ” QUÃ‰ TENEMOS (Estado Actual)

### Input Actual (Confuso)

**MÃºltiples archivos de entrada:**
1. `processed_clean`: `/Users/cesaresparza/.../final_processed_data_CLEAN.csv`
   - Usado por: Step 1 (paneles B, E, F, G)
   
2. `raw`: `/Users/cesaresparza/.../miRNA_count.Q33.txt`
   - Usado por: Step 1 (paneles C, D)
   
3. `step1_original`: `/Users/cesaresparza/.../step1_original_data.csv`
   - Usado por: Step 1.5 (necesita SNV + total counts)

**Problemas:**
- âŒ Rutas hardcodeadas (absolutas, usuario-especÃ­ficas)
- âŒ MÃºltiples inputs en lugar de uno solo
- âŒ No claro cuÃ¡l es el "input principal"
- âŒ Usuario debe editar `config.yaml` manualmente

### EjecuciÃ³n Actual

**Comandos disponibles:**
```bash
# OpciÃ³n 1: Snakemake directo
snakemake -j 4

# OpciÃ³n 2: Por pasos
snakemake -j 4 all_step1
snakemake -j 1 all_step1_5

# OpciÃ³n 3: Panel individual
snakemake -j 1 outputs/step1/figures/step1_panelB_*.png
```

**Problemas:**
- âš ï¸ Requiere editar `config.yaml` antes de ejecutar
- âš ï¸ No hay script simple `run.sh` funcional aÃºn
- âš ï¸ No hay validaciÃ³n de input automÃ¡tica
- âœ… Snakemake funciona correctamente
- âœ… ParalelizaciÃ³n funciona

### Output Actual

**Genera correctamente:**
- âœ… Step 1: 6 figuras + 6 tablas + viewer HTML
- âœ… Step 1.5: 11 figuras + 7 tablas + viewer HTML
- âœ… Step 2: Estructura lista pero no completado
- âœ… Viewers HTML funcionan

**Estructura:**
```
outputs/
â”œâ”€â”€ step1/
â”‚   â”œâ”€â”€ figures/ (6 PNGs)
â”‚   â”œâ”€â”€ tables/ (6 CSVs)
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ step1_5/
â”‚   â”œâ”€â”€ figures/ (11 PNGs)
â”‚   â”œâ”€â”€ tables/ (7 CSVs)
â”‚   â””â”€â”€ logs/
â””â”€â”€ step2/ (vacÃ­o por ahora)
```

**Problemas:**
- âš ï¸ Outputs estÃ¡n bien organizados pero faltan algunos pasos

---

## ğŸ“‹ GAPS (Diferencias)

### Gap 1: Input âŒ

**Queremos:**
```
Un solo archivo CSV â†’ Pipeline procesa todo
```

**Tenemos:**
```
3 archivos diferentes con rutas hardcodeadas
Usuario debe editar config.yaml manualmente
```

**Gap:**
- Falta unificaciÃ³n de inputs
- Falta auto-configuraciÃ³n
- Falta validaciÃ³n de formato

---

### Gap 2: EjecuciÃ³n âš ï¸

**Queremos:**
```
./run.sh input.csv  â†’ Todo funciona automÃ¡ticamente
```

**Tenemos:**
```
1. Editar config.yaml con rutas
2. snakemake -j 4
```

**Gap:**
- `run.sh` existe pero no actualiza config automÃ¡ticamente
- Falta validaciÃ³n de input antes de ejecutar
- Falta manejo de errores claro

---

### Gap 3: Genericidad âš ï¸

**Queremos:**
```
Pipeline genÃ©rico que funciona con cualquier dataset
```

**Tenemos:**
```
Rutas hardcodeadas a archivos especÃ­ficos del usuario
Nombres de columnas asumidos pero no validados
```

**Gap:**
- Pipeline funciona pero no es portable
- Falta detecciÃ³n automÃ¡tica de formato
- Falta documentaciÃ³n clara del formato esperado

---

### Gap 4: Step 2 ğŸ“‹

**Queremos:**
```
Pipeline completo: Step 1 + Step 1.5 + Step 2
```

**Tenemos:**
```
Step 1 âœ… Completo
Step 1.5 âœ… Completo
Step 2 âŒ Incompleto (solo estructura)
```

**Gap:**
- Falta completar Step 2 (comparaciones grupo vs grupo)

---

## ğŸ¯ DECISIONES NECESARIAS

### Pregunta 1: Â¿CuÃ¡l debe ser el INPUT PRINCIPAL?

**OpciÃ³n A: Archivo RAW original**
- Input: `miRNA_count.Q33.txt`
- Pipeline hace: split, collapse, procesamiento, anÃ¡lisis
- **Pro:** MÃ¡s genÃ©rico, empieza desde raw
- **Contra:** MÃ¡s lento, requiere mÃ¡s procesamiento

**OpciÃ³n B: Archivo PROCESADO (split-collapse)**
- Input: `step1_original_data.csv` (ya procesado)
- Pipeline hace: anÃ¡lisis directo
- **Pro:** MÃ¡s rÃ¡pido, usuario ya procesÃ³ datos
- **Contra:** Asume formato especÃ­fico

**OpciÃ³n C: Ambos (auto-detecciÃ³n)**
- Pipeline detecta si es raw o processed
- **Pro:** MÃ¡s flexible
- **Contra:** MÃ¡s complejo de implementar

**âš ï¸ NECESITAMOS DECIDIR:** Â¿CuÃ¡l queremos?

---

### Pregunta 2: Â¿CÃ³mo manejamos metadata (grupos)?

**Estado actual:**
- Step 1 y 1.5: No requieren metadata (funcionan sin grupos)
- Step 2: Requiere metadata (comparaciÃ³n ALS vs Control)

**Opciones:**

**OpciÃ³n A: Input opcional**
```bash
./run.sh input.csv [metadata.csv]  # metadata opcional
```

**OpciÃ³n B: Auto-detecciÃ³n**
- Pipeline busca metadata en directorio
- Si no encuentra, solo ejecuta Step 1 + 1.5

**OpciÃ³n C: ConfiguraciÃ³n manual**
- Usuario edita config.yaml para metadata
- MÃ¡s explÃ­cito pero menos automÃ¡tico

**âš ï¸ NECESITAMOS DECIDIR:** Â¿CÃ³mo lo manejamos?

---

### Pregunta 3: Â¿QuÃ© debe hacer el pipeline automÃ¡ticamente?

**Opciones:**

**Nivel 1: MÃ­nimo (actual)**
- Usuario edita config.yaml
- Ejecuta snakemake
- âœ… Funciona pero requiere configuraciÃ³n manual

**Nivel 2: Intermedio (propuesto)**
- Usuario pasa input como argumento
- run.sh actualiza config.yaml automÃ¡ticamente
- Ejecuta pipeline
- âœ… MÃ¡s simple pero aÃºn requiere entender estructura

**Nivel 3: MÃ¡ximo (ideal)**
- Usuario pasa input
- Pipeline valida formato
- Pipeline auto-detecta tipo (raw/processed)
- Pipeline decide quÃ© steps ejecutar
- Pipeline genera todo automÃ¡ticamente
- âœ… MÃ¡ximo automatismo pero mÃ¡s complejo

**âš ï¸ NECESITAMOS DECIDIR:** Â¿Hasta dÃ³nde queremos automatizar?

---

## ğŸ’¡ RECOMENDACIONES

### RecomendaciÃ³n 1: Unificar Input (PRIORIDAD ALTA)

**Proponer:**
- Input principal: Archivo procesado (split-collapse)
- Formato: CSV con columnas `miRNA name`, `pos:mut`, y columnas de muestra
- Pipeline asume que datos ya estÃ¡n en formato correcto

**RazÃ³n:**
- MÃ¡s rÃ¡pido (no procesa raw)
- Usuario controla pre-procesamiento
- MÃ¡s simple de validar

---

### RecomendaciÃ³n 2: Auto-configuraciÃ³n Simple (PRIORIDAD MEDIA)

**Proponer:**
```bash
./run.sh input.csv  # Actualiza config.yaml y ejecuta
```

**Implementar:**
- `run.sh` toma input como argumento
- Actualiza `config.yaml` automÃ¡ticamente
- Valida que archivo existe
- Ejecuta pipeline

**RazÃ³n:**
- Balance entre simplicidad y control
- Usuario no edita YAML manualmente
- Pero puede si quiere (config.yaml sigue siendo editable)

---

### RecomendaciÃ³n 3: Metadata Opcional (PRIORIDAD MEDIA)

**Proponer:**
```bash
./run.sh input.csv [metadata.csv]  # metadata opcional
```

**Comportamiento:**
- Sin metadata: Ejecuta Step 1 + Step 1.5 (solo anÃ¡lisis exploratorio)
- Con metadata: Ejecuta Step 1 + Step 1.5 + Step 2 (comparaciones)

**RazÃ³n:**
- Permite usar pipeline sin grupos
- Agrega funcionalidad cuando metadata disponible
- Flexible y claro

---

### RecomendaciÃ³n 4: Completar Step 2 (PRIORIDAD BAJA)

**Proponer:**
- Step 2 como fase futura
- Por ahora: Step 1 + Step 1.5 completos y funcionando

**RazÃ³n:**
- Step 1 y 1.5 ya funcionan bien
- Step 2 requiere decisiones sobre metadata
- Mejor consolidar lo que funciona primero

---

## ğŸ“Š PLAN DE ACCIÃ“N PROPUESTO

### Fase 1: Unificar y Simplificar Input âš¡
1. Decidir: Â¿raw o processed como input principal?
2. Simplificar config.yaml: Solo una ruta de input
3. Actualizar todas las reglas para usar el mismo input

### Fase 2: Auto-configuraciÃ³n ğŸ”§
1. Implementar auto-actualizaciÃ³n en run.sh
2. ValidaciÃ³n bÃ¡sica de input (existe, formato CSV)
3. Mensajes de error claros

### Fase 3: ValidaciÃ³n y DocumentaciÃ³n ğŸ“š
1. Script de validaciÃ³n de formato
2. README actualizado con formato esperado
3. Ejemplo de datos pequeÃ±o incluido

### Fase 4: Metadata Opcional (futuro) ğŸ”®
1. Implementar metadata como argumento opcional
2. Auto-detecciÃ³n de grupos
3. Step 2 condicional basado en metadata

---

## â“ PREGUNTAS PARA DISCUTIR

1. **Â¿Raw o Processed como input principal?**
   - Â¿Prefieres que usuario procese datos primero?
   - Â¿O prefieres pipeline completo desde raw?

2. **Â¿QuÃ© tan automÃ¡tico queremos?**
   - Â¿Solo auto-config o tambiÃ©n auto-detecciÃ³n de formato?

3. **Â¿Metadata cÃ³mo lo manejamos?**
   - Â¿Opcional como argumento?
   - Â¿O parte de configuraciÃ³n?

4. **Â¿Prioridad de pasos?**
   - Â¿Primero consolidar Step 1 + 1.5?
   - Â¿O empezar a trabajar en Step 2?

---

**Estado:** Documento para discusiÃ³n  
**PrÃ³ximo paso:** Decidir respuestas a preguntas antes de implementar


**Fecha:** 2025-11-01  
**Objetivo:** Comparar el objetivo final con la realidad actual para identificar gaps y planificar mejoras

---

## ğŸ¯ QUÃ‰ QUEREMOS (Objetivo Final)

### VisiÃ³n Ideal
Un pipeline **simple y directo** como los pipelines estÃ¡ndar de GitHub (nf-core, skipper, etc.):

```
INPUT: Un archivo CSV
  â†“
./run.sh input.csv
  â†“
OUTPUTS: Todas las grÃ¡ficas + tablas + viewers HTML
```

### CaracterÃ­sticas Deseadas

1. **Input Simple y Ãšnico**
   - Un solo archivo CSV como entrada
   - Formato bien documentado
   - ValidaciÃ³n automÃ¡tica del formato

2. **EjecuciÃ³n Simple**
   - Un comando: `./run.sh input.csv`
   - Sin configuraciÃ³n manual necesaria
   - Auto-detecciÃ³n de parÃ¡metros

3. **Output Completo**
   - Todas las figuras (Step 1 + Step 1.5 + Step 2)
   - Todas las tablas CSV
   - Viewers HTML interactivos
   - Todo en directorio organizado

4. **Pipeline GenÃ©rico**
   - Funciona con cualquier dataset (ALS + Control)
   - No hardcodea rutas especÃ­ficas
   - Configurable pero con defaults sensatos

---

## ğŸ” QUÃ‰ TENEMOS (Estado Actual)

### Input Actual (Confuso)

**MÃºltiples archivos de entrada:**
1. `processed_clean`: `/Users/cesaresparza/.../final_processed_data_CLEAN.csv`
   - Usado por: Step 1 (paneles B, E, F, G)
   
2. `raw`: `/Users/cesaresparza/.../miRNA_count.Q33.txt`
   - Usado por: Step 1 (paneles C, D)
   
3. `step1_original`: `/Users/cesaresparza/.../step1_original_data.csv`
   - Usado por: Step 1.5 (necesita SNV + total counts)

**Problemas:**
- âŒ Rutas hardcodeadas (absolutas, usuario-especÃ­ficas)
- âŒ MÃºltiples inputs en lugar de uno solo
- âŒ No claro cuÃ¡l es el "input principal"
- âŒ Usuario debe editar `config.yaml` manualmente

### EjecuciÃ³n Actual

**Comandos disponibles:**
```bash
# OpciÃ³n 1: Snakemake directo
snakemake -j 4

# OpciÃ³n 2: Por pasos
snakemake -j 4 all_step1
snakemake -j 1 all_step1_5

# OpciÃ³n 3: Panel individual
snakemake -j 1 outputs/step1/figures/step1_panelB_*.png
```

**Problemas:**
- âš ï¸ Requiere editar `config.yaml` antes de ejecutar
- âš ï¸ No hay script simple `run.sh` funcional aÃºn
- âš ï¸ No hay validaciÃ³n de input automÃ¡tica
- âœ… Snakemake funciona correctamente
- âœ… ParalelizaciÃ³n funciona

### Output Actual

**Genera correctamente:**
- âœ… Step 1: 6 figuras + 6 tablas + viewer HTML
- âœ… Step 1.5: 11 figuras + 7 tablas + viewer HTML
- âœ… Step 2: Estructura lista pero no completado
- âœ… Viewers HTML funcionan

**Estructura:**
```
outputs/
â”œâ”€â”€ step1/
â”‚   â”œâ”€â”€ figures/ (6 PNGs)
â”‚   â”œâ”€â”€ tables/ (6 CSVs)
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ step1_5/
â”‚   â”œâ”€â”€ figures/ (11 PNGs)
â”‚   â”œâ”€â”€ tables/ (7 CSVs)
â”‚   â””â”€â”€ logs/
â””â”€â”€ step2/ (vacÃ­o por ahora)
```

**Problemas:**
- âš ï¸ Outputs estÃ¡n bien organizados pero faltan algunos pasos

---

## ğŸ“‹ GAPS (Diferencias)

### Gap 1: Input âŒ

**Queremos:**
```
Un solo archivo CSV â†’ Pipeline procesa todo
```

**Tenemos:**
```
3 archivos diferentes con rutas hardcodeadas
Usuario debe editar config.yaml manualmente
```

**Gap:**
- Falta unificaciÃ³n de inputs
- Falta auto-configuraciÃ³n
- Falta validaciÃ³n de formato

---

### Gap 2: EjecuciÃ³n âš ï¸

**Queremos:**
```
./run.sh input.csv  â†’ Todo funciona automÃ¡ticamente
```

**Tenemos:**
```
1. Editar config.yaml con rutas
2. snakemake -j 4
```

**Gap:**
- `run.sh` existe pero no actualiza config automÃ¡ticamente
- Falta validaciÃ³n de input antes de ejecutar
- Falta manejo de errores claro

---

### Gap 3: Genericidad âš ï¸

**Queremos:**
```
Pipeline genÃ©rico que funciona con cualquier dataset
```

**Tenemos:**
```
Rutas hardcodeadas a archivos especÃ­ficos del usuario
Nombres de columnas asumidos pero no validados
```

**Gap:**
- Pipeline funciona pero no es portable
- Falta detecciÃ³n automÃ¡tica de formato
- Falta documentaciÃ³n clara del formato esperado

---

### Gap 4: Step 2 ğŸ“‹

**Queremos:**
```
Pipeline completo: Step 1 + Step 1.5 + Step 2
```

**Tenemos:**
```
Step 1 âœ… Completo
Step 1.5 âœ… Completo
Step 2 âŒ Incompleto (solo estructura)
```

**Gap:**
- Falta completar Step 2 (comparaciones grupo vs grupo)

---

## ğŸ¯ DECISIONES NECESARIAS

### Pregunta 1: Â¿CuÃ¡l debe ser el INPUT PRINCIPAL?

**OpciÃ³n A: Archivo RAW original**
- Input: `miRNA_count.Q33.txt`
- Pipeline hace: split, collapse, procesamiento, anÃ¡lisis
- **Pro:** MÃ¡s genÃ©rico, empieza desde raw
- **Contra:** MÃ¡s lento, requiere mÃ¡s procesamiento

**OpciÃ³n B: Archivo PROCESADO (split-collapse)**
- Input: `step1_original_data.csv` (ya procesado)
- Pipeline hace: anÃ¡lisis directo
- **Pro:** MÃ¡s rÃ¡pido, usuario ya procesÃ³ datos
- **Contra:** Asume formato especÃ­fico

**OpciÃ³n C: Ambos (auto-detecciÃ³n)**
- Pipeline detecta si es raw o processed
- **Pro:** MÃ¡s flexible
- **Contra:** MÃ¡s complejo de implementar

**âš ï¸ NECESITAMOS DECIDIR:** Â¿CuÃ¡l queremos?

---

### Pregunta 2: Â¿CÃ³mo manejamos metadata (grupos)?

**Estado actual:**
- Step 1 y 1.5: No requieren metadata (funcionan sin grupos)
- Step 2: Requiere metadata (comparaciÃ³n ALS vs Control)

**Opciones:**

**OpciÃ³n A: Input opcional**
```bash
./run.sh input.csv [metadata.csv]  # metadata opcional
```

**OpciÃ³n B: Auto-detecciÃ³n**
- Pipeline busca metadata en directorio
- Si no encuentra, solo ejecuta Step 1 + 1.5

**OpciÃ³n C: ConfiguraciÃ³n manual**
- Usuario edita config.yaml para metadata
- MÃ¡s explÃ­cito pero menos automÃ¡tico

**âš ï¸ NECESITAMOS DECIDIR:** Â¿CÃ³mo lo manejamos?

---

### Pregunta 3: Â¿QuÃ© debe hacer el pipeline automÃ¡ticamente?

**Opciones:**

**Nivel 1: MÃ­nimo (actual)**
- Usuario edita config.yaml
- Ejecuta snakemake
- âœ… Funciona pero requiere configuraciÃ³n manual

**Nivel 2: Intermedio (propuesto)**
- Usuario pasa input como argumento
- run.sh actualiza config.yaml automÃ¡ticamente
- Ejecuta pipeline
- âœ… MÃ¡s simple pero aÃºn requiere entender estructura

**Nivel 3: MÃ¡ximo (ideal)**
- Usuario pasa input
- Pipeline valida formato
- Pipeline auto-detecta tipo (raw/processed)
- Pipeline decide quÃ© steps ejecutar
- Pipeline genera todo automÃ¡ticamente
- âœ… MÃ¡ximo automatismo pero mÃ¡s complejo

**âš ï¸ NECESITAMOS DECIDIR:** Â¿Hasta dÃ³nde queremos automatizar?

---

## ğŸ’¡ RECOMENDACIONES

### RecomendaciÃ³n 1: Unificar Input (PRIORIDAD ALTA)

**Proponer:**
- Input principal: Archivo procesado (split-collapse)
- Formato: CSV con columnas `miRNA name`, `pos:mut`, y columnas de muestra
- Pipeline asume que datos ya estÃ¡n en formato correcto

**RazÃ³n:**
- MÃ¡s rÃ¡pido (no procesa raw)
- Usuario controla pre-procesamiento
- MÃ¡s simple de validar

---

### RecomendaciÃ³n 2: Auto-configuraciÃ³n Simple (PRIORIDAD MEDIA)

**Proponer:**
```bash
./run.sh input.csv  # Actualiza config.yaml y ejecuta
```

**Implementar:**
- `run.sh` toma input como argumento
- Actualiza `config.yaml` automÃ¡ticamente
- Valida que archivo existe
- Ejecuta pipeline

**RazÃ³n:**
- Balance entre simplicidad y control
- Usuario no edita YAML manualmente
- Pero puede si quiere (config.yaml sigue siendo editable)

---

### RecomendaciÃ³n 3: Metadata Opcional (PRIORIDAD MEDIA)

**Proponer:**
```bash
./run.sh input.csv [metadata.csv]  # metadata opcional
```

**Comportamiento:**
- Sin metadata: Ejecuta Step 1 + Step 1.5 (solo anÃ¡lisis exploratorio)
- Con metadata: Ejecuta Step 1 + Step 1.5 + Step 2 (comparaciones)

**RazÃ³n:**
- Permite usar pipeline sin grupos
- Agrega funcionalidad cuando metadata disponible
- Flexible y claro

---

### RecomendaciÃ³n 4: Completar Step 2 (PRIORIDAD BAJA)

**Proponer:**
- Step 2 como fase futura
- Por ahora: Step 1 + Step 1.5 completos y funcionando

**RazÃ³n:**
- Step 1 y 1.5 ya funcionan bien
- Step 2 requiere decisiones sobre metadata
- Mejor consolidar lo que funciona primero

---

## ğŸ“Š PLAN DE ACCIÃ“N PROPUESTO

### Fase 1: Unificar y Simplificar Input âš¡
1. Decidir: Â¿raw o processed como input principal?
2. Simplificar config.yaml: Solo una ruta de input
3. Actualizar todas las reglas para usar el mismo input

### Fase 2: Auto-configuraciÃ³n ğŸ”§
1. Implementar auto-actualizaciÃ³n en run.sh
2. ValidaciÃ³n bÃ¡sica de input (existe, formato CSV)
3. Mensajes de error claros

### Fase 3: ValidaciÃ³n y DocumentaciÃ³n ğŸ“š
1. Script de validaciÃ³n de formato
2. README actualizado con formato esperado
3. Ejemplo de datos pequeÃ±o incluido

### Fase 4: Metadata Opcional (futuro) ğŸ”®
1. Implementar metadata como argumento opcional
2. Auto-detecciÃ³n de grupos
3. Step 2 condicional basado en metadata

---

## â“ PREGUNTAS PARA DISCUTIR

1. **Â¿Raw o Processed como input principal?**
   - Â¿Prefieres que usuario procese datos primero?
   - Â¿O prefieres pipeline completo desde raw?

2. **Â¿QuÃ© tan automÃ¡tico queremos?**
   - Â¿Solo auto-config o tambiÃ©n auto-detecciÃ³n de formato?

3. **Â¿Metadata cÃ³mo lo manejamos?**
   - Â¿Opcional como argumento?
   - Â¿O parte de configuraciÃ³n?

4. **Â¿Prioridad de pasos?**
   - Â¿Primero consolidar Step 1 + 1.5?
   - Â¿O empezar a trabajar en Step 2?

---

**Estado:** Documento para discusiÃ³n  
**PrÃ³ximo paso:** Decidir respuestas a preguntas antes de implementar


**Fecha:** 2025-11-01  
**Objetivo:** Comparar el objetivo final con la realidad actual para identificar gaps y planificar mejoras

---

## ğŸ¯ QUÃ‰ QUEREMOS (Objetivo Final)

### VisiÃ³n Ideal
Un pipeline **simple y directo** como los pipelines estÃ¡ndar de GitHub (nf-core, skipper, etc.):

```
INPUT: Un archivo CSV
  â†“
./run.sh input.csv
  â†“
OUTPUTS: Todas las grÃ¡ficas + tablas + viewers HTML
```

### CaracterÃ­sticas Deseadas

1. **Input Simple y Ãšnico**
   - Un solo archivo CSV como entrada
   - Formato bien documentado
   - ValidaciÃ³n automÃ¡tica del formato

2. **EjecuciÃ³n Simple**
   - Un comando: `./run.sh input.csv`
   - Sin configuraciÃ³n manual necesaria
   - Auto-detecciÃ³n de parÃ¡metros

3. **Output Completo**
   - Todas las figuras (Step 1 + Step 1.5 + Step 2)
   - Todas las tablas CSV
   - Viewers HTML interactivos
   - Todo en directorio organizado

4. **Pipeline GenÃ©rico**
   - Funciona con cualquier dataset (ALS + Control)
   - No hardcodea rutas especÃ­ficas
   - Configurable pero con defaults sensatos

---

## ğŸ” QUÃ‰ TENEMOS (Estado Actual)

### Input Actual (Confuso)

**MÃºltiples archivos de entrada:**
1. `processed_clean`: `/Users/cesaresparza/.../final_processed_data_CLEAN.csv`
   - Usado por: Step 1 (paneles B, E, F, G)
   
2. `raw`: `/Users/cesaresparza/.../miRNA_count.Q33.txt`
   - Usado por: Step 1 (paneles C, D)
   
3. `step1_original`: `/Users/cesaresparza/.../step1_original_data.csv`
   - Usado por: Step 1.5 (necesita SNV + total counts)

**Problemas:**
- âŒ Rutas hardcodeadas (absolutas, usuario-especÃ­ficas)
- âŒ MÃºltiples inputs en lugar de uno solo
- âŒ No claro cuÃ¡l es el "input principal"
- âŒ Usuario debe editar `config.yaml` manualmente

### EjecuciÃ³n Actual

**Comandos disponibles:**
```bash
# OpciÃ³n 1: Snakemake directo
snakemake -j 4

# OpciÃ³n 2: Por pasos
snakemake -j 4 all_step1
snakemake -j 1 all_step1_5

# OpciÃ³n 3: Panel individual
snakemake -j 1 outputs/step1/figures/step1_panelB_*.png
```

**Problemas:**
- âš ï¸ Requiere editar `config.yaml` antes de ejecutar
- âš ï¸ No hay script simple `run.sh` funcional aÃºn
- âš ï¸ No hay validaciÃ³n de input automÃ¡tica
- âœ… Snakemake funciona correctamente
- âœ… ParalelizaciÃ³n funciona

### Output Actual

**Genera correctamente:**
- âœ… Step 1: 6 figuras + 6 tablas + viewer HTML
- âœ… Step 1.5: 11 figuras + 7 tablas + viewer HTML
- âœ… Step 2: Estructura lista pero no completado
- âœ… Viewers HTML funcionan

**Estructura:**
```
outputs/
â”œâ”€â”€ step1/
â”‚   â”œâ”€â”€ figures/ (6 PNGs)
â”‚   â”œâ”€â”€ tables/ (6 CSVs)
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ step1_5/
â”‚   â”œâ”€â”€ figures/ (11 PNGs)
â”‚   â”œâ”€â”€ tables/ (7 CSVs)
â”‚   â””â”€â”€ logs/
â””â”€â”€ step2/ (vacÃ­o por ahora)
```

**Problemas:**
- âš ï¸ Outputs estÃ¡n bien organizados pero faltan algunos pasos

---

## ğŸ“‹ GAPS (Diferencias)

### Gap 1: Input âŒ

**Queremos:**
```
Un solo archivo CSV â†’ Pipeline procesa todo
```

**Tenemos:**
```
3 archivos diferentes con rutas hardcodeadas
Usuario debe editar config.yaml manualmente
```

**Gap:**
- Falta unificaciÃ³n de inputs
- Falta auto-configuraciÃ³n
- Falta validaciÃ³n de formato

---

### Gap 2: EjecuciÃ³n âš ï¸

**Queremos:**
```
./run.sh input.csv  â†’ Todo funciona automÃ¡ticamente
```

**Tenemos:**
```
1. Editar config.yaml con rutas
2. snakemake -j 4
```

**Gap:**
- `run.sh` existe pero no actualiza config automÃ¡ticamente
- Falta validaciÃ³n de input antes de ejecutar
- Falta manejo de errores claro

---

### Gap 3: Genericidad âš ï¸

**Queremos:**
```
Pipeline genÃ©rico que funciona con cualquier dataset
```

**Tenemos:**
```
Rutas hardcodeadas a archivos especÃ­ficos del usuario
Nombres de columnas asumidos pero no validados
```

**Gap:**
- Pipeline funciona pero no es portable
- Falta detecciÃ³n automÃ¡tica de formato
- Falta documentaciÃ³n clara del formato esperado

---

### Gap 4: Step 2 ğŸ“‹

**Queremos:**
```
Pipeline completo: Step 1 + Step 1.5 + Step 2
```

**Tenemos:**
```
Step 1 âœ… Completo
Step 1.5 âœ… Completo
Step 2 âŒ Incompleto (solo estructura)
```

**Gap:**
- Falta completar Step 2 (comparaciones grupo vs grupo)

---

## ğŸ¯ DECISIONES NECESARIAS

### Pregunta 1: Â¿CuÃ¡l debe ser el INPUT PRINCIPAL?

**OpciÃ³n A: Archivo RAW original**
- Input: `miRNA_count.Q33.txt`
- Pipeline hace: split, collapse, procesamiento, anÃ¡lisis
- **Pro:** MÃ¡s genÃ©rico, empieza desde raw
- **Contra:** MÃ¡s lento, requiere mÃ¡s procesamiento

**OpciÃ³n B: Archivo PROCESADO (split-collapse)**
- Input: `step1_original_data.csv` (ya procesado)
- Pipeline hace: anÃ¡lisis directo
- **Pro:** MÃ¡s rÃ¡pido, usuario ya procesÃ³ datos
- **Contra:** Asume formato especÃ­fico

**OpciÃ³n C: Ambos (auto-detecciÃ³n)**
- Pipeline detecta si es raw o processed
- **Pro:** MÃ¡s flexible
- **Contra:** MÃ¡s complejo de implementar

**âš ï¸ NECESITAMOS DECIDIR:** Â¿CuÃ¡l queremos?

---

### Pregunta 2: Â¿CÃ³mo manejamos metadata (grupos)?

**Estado actual:**
- Step 1 y 1.5: No requieren metadata (funcionan sin grupos)
- Step 2: Requiere metadata (comparaciÃ³n ALS vs Control)

**Opciones:**

**OpciÃ³n A: Input opcional**
```bash
./run.sh input.csv [metadata.csv]  # metadata opcional
```

**OpciÃ³n B: Auto-detecciÃ³n**
- Pipeline busca metadata en directorio
- Si no encuentra, solo ejecuta Step 1 + 1.5

**OpciÃ³n C: ConfiguraciÃ³n manual**
- Usuario edita config.yaml para metadata
- MÃ¡s explÃ­cito pero menos automÃ¡tico

**âš ï¸ NECESITAMOS DECIDIR:** Â¿CÃ³mo lo manejamos?

---

### Pregunta 3: Â¿QuÃ© debe hacer el pipeline automÃ¡ticamente?

**Opciones:**

**Nivel 1: MÃ­nimo (actual)**
- Usuario edita config.yaml
- Ejecuta snakemake
- âœ… Funciona pero requiere configuraciÃ³n manual

**Nivel 2: Intermedio (propuesto)**
- Usuario pasa input como argumento
- run.sh actualiza config.yaml automÃ¡ticamente
- Ejecuta pipeline
- âœ… MÃ¡s simple pero aÃºn requiere entender estructura

**Nivel 3: MÃ¡ximo (ideal)**
- Usuario pasa input
- Pipeline valida formato
- Pipeline auto-detecta tipo (raw/processed)
- Pipeline decide quÃ© steps ejecutar
- Pipeline genera todo automÃ¡ticamente
- âœ… MÃ¡ximo automatismo pero mÃ¡s complejo

**âš ï¸ NECESITAMOS DECIDIR:** Â¿Hasta dÃ³nde queremos automatizar?

---

## ğŸ’¡ RECOMENDACIONES

### RecomendaciÃ³n 1: Unificar Input (PRIORIDAD ALTA)

**Proponer:**
- Input principal: Archivo procesado (split-collapse)
- Formato: CSV con columnas `miRNA name`, `pos:mut`, y columnas de muestra
- Pipeline asume que datos ya estÃ¡n en formato correcto

**RazÃ³n:**
- MÃ¡s rÃ¡pido (no procesa raw)
- Usuario controla pre-procesamiento
- MÃ¡s simple de validar

---

### RecomendaciÃ³n 2: Auto-configuraciÃ³n Simple (PRIORIDAD MEDIA)

**Proponer:**
```bash
./run.sh input.csv  # Actualiza config.yaml y ejecuta
```

**Implementar:**
- `run.sh` toma input como argumento
- Actualiza `config.yaml` automÃ¡ticamente
- Valida que archivo existe
- Ejecuta pipeline

**RazÃ³n:**
- Balance entre simplicidad y control
- Usuario no edita YAML manualmente
- Pero puede si quiere (config.yaml sigue siendo editable)

---

### RecomendaciÃ³n 3: Metadata Opcional (PRIORIDAD MEDIA)

**Proponer:**
```bash
./run.sh input.csv [metadata.csv]  # metadata opcional
```

**Comportamiento:**
- Sin metadata: Ejecuta Step 1 + Step 1.5 (solo anÃ¡lisis exploratorio)
- Con metadata: Ejecuta Step 1 + Step 1.5 + Step 2 (comparaciones)

**RazÃ³n:**
- Permite usar pipeline sin grupos
- Agrega funcionalidad cuando metadata disponible
- Flexible y claro

---

### RecomendaciÃ³n 4: Completar Step 2 (PRIORIDAD BAJA)

**Proponer:**
- Step 2 como fase futura
- Por ahora: Step 1 + Step 1.5 completos y funcionando

**RazÃ³n:**
- Step 1 y 1.5 ya funcionan bien
- Step 2 requiere decisiones sobre metadata
- Mejor consolidar lo que funciona primero

---

## ğŸ“Š PLAN DE ACCIÃ“N PROPUESTO

### Fase 1: Unificar y Simplificar Input âš¡
1. Decidir: Â¿raw o processed como input principal?
2. Simplificar config.yaml: Solo una ruta de input
3. Actualizar todas las reglas para usar el mismo input

### Fase 2: Auto-configuraciÃ³n ğŸ”§
1. Implementar auto-actualizaciÃ³n en run.sh
2. ValidaciÃ³n bÃ¡sica de input (existe, formato CSV)
3. Mensajes de error claros

### Fase 3: ValidaciÃ³n y DocumentaciÃ³n ğŸ“š
1. Script de validaciÃ³n de formato
2. README actualizado con formato esperado
3. Ejemplo de datos pequeÃ±o incluido

### Fase 4: Metadata Opcional (futuro) ğŸ”®
1. Implementar metadata como argumento opcional
2. Auto-detecciÃ³n de grupos
3. Step 2 condicional basado en metadata

---

## â“ PREGUNTAS PARA DISCUTIR

1. **Â¿Raw o Processed como input principal?**
   - Â¿Prefieres que usuario procese datos primero?
   - Â¿O prefieres pipeline completo desde raw?

2. **Â¿QuÃ© tan automÃ¡tico queremos?**
   - Â¿Solo auto-config o tambiÃ©n auto-detecciÃ³n de formato?

3. **Â¿Metadata cÃ³mo lo manejamos?**
   - Â¿Opcional como argumento?
   - Â¿O parte de configuraciÃ³n?

4. **Â¿Prioridad de pasos?**
   - Â¿Primero consolidar Step 1 + 1.5?
   - Â¿O empezar a trabajar en Step 2?

---

**Estado:** Documento para discusiÃ³n  
**PrÃ³ximo paso:** Decidir respuestas a preguntas antes de implementar

